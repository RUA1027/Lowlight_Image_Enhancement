# 数学原理阐释与项目底层

## 数学原理

### **第一部分：所有反向传播的起点 —— 链式法则与雅可比矩阵**

我们先回归到任何神经网络反向传播的最根本原理：**多元微积分的链式法则**。

想象一个最简单的网络层，它接受一个输入向量 `s`，经过一个函数 `f` 的变换，得到一个输出向量 `y`。

这个 `y` 再经过后续网络计算，最终产生一个标量损失 `L`。
$$
s → f(s) → y → ... → L
$$
在反向传播中，我们的核心任务是：已知**损失 `L` 相对于输出 `y` 的梯度 `∂L/∂y`**（这是从下一层传回来的梯度，我们称之为 `grad_output`），我们想要求得**损失 `L` 相对于输入 `s` 的梯度 `∂L/∂s`**（我们需要将这个梯度继续向前传递，我们称之为 `grad_input`）。

根据链式法则，它们的关系是：
$$
\frac{\partial L}{\partial \mathbf{s}} = \left(\frac{\partial \mathbf{y}}{\partial \mathbf{s}}\right)^T \cdot \frac{\partial L}{\partial \mathbf{y}}
$$
我们来解读这个公式的核心：

- **`∂y/∂s`**：这就是描述了 `f` 函数局部线性行为的**雅可比矩阵 `J`**。它的每一个元素 `J_ij` 代表了输出 `y_i` 对输入 `s_j` 的偏导数。这个矩阵完整地刻画了输入 `s` 的微小变化会如何在输出 `y` 上产生影响。
  $$
  \mathbf{J} = \frac{\partial \mathbf{y}}{\partial \mathbf{s}} =
  \begin{bmatrix}
  \frac{\partial y_1}{\partial s_1} & \cdots & \frac{\partial y_1}{\partial s_n} \\
  \vdots & \ddots & \vdots \\
  \frac{\partial y_m}{\partial s_1} & \cdots & \frac{\partial y_m}{\partial s_n}
  \end{bmatrix}
  $$

- **(`∂y/∂s`)ᵀ**：即雅可比矩阵 `J` 的**转置**。这个转置操作并非凭空出现，它是向量微积分中进行维度匹配和正确投影的必然要求。

所以，任何一层反向传播的数学本质都可以写成：
$$
grad_{input} = J^T · grad_{output}
$$
**NewBP算法的核心，就是坚持在梯度计算中，必须严格遵循这个由雅可-比矩阵转置主导的法则。**

------

### 第二部分：卷积操作的“真面目” —— 一个特殊的雅可比矩阵

现在，我们面临的关键问题是：对于一个**二维卷积操作**，它的雅可比矩阵 `J` 长什么样？

虽然卷积在代码中表现为高效的滑动窗口运算，但从数学上看，它依然是一个**线性变换**。任何线性变换，都可以表示为一次矩阵乘法。

让我们用一个极简的**一维卷积**为例，这样更容易看清矩阵的结构：

- 输入 `s` = `[s₁, s₂, s₃, s₄]`
- 卷积核 `K` = `[k₁, k₂]`
- 输出 `y` = `[y₁, y₂]` （假设padding=0, stride=1）

根据卷积定义：

- `y₁ = k₁·s₁ + k₂·s₂ + 0·s₃ + 0·s₄`
- `y₂ = 0·s₁ + k₁·s₂ + k₂·s₃ + 0·s₄`

我们可以把这个关系写成矩阵乘法 `y = J · s` 的形式：
$$
\begin{bmatrix} y_1 \\ y_2 \end{bmatrix}
=
\begin{bmatrix}
k_1 & k_2 & 0 & 0 \\
0 & k_1 & k_2 & 0
\end{bmatrix}
\begin{bmatrix} s_1 \\ s_2 \\ s_3 \\ s_4 \end{bmatrix}
$$
上面这个 `[[k₁, k₂, 0, 0], [0, k₁, k₂, 0]]` 的矩阵，**就是这次一维卷积操作的雅可比矩阵 `J`**。

对于二维卷积，原理完全相同，只是这个 `J` 矩阵会变得极其巨大和复杂，但它的所有非零元素，依然完全由那个小小的二维卷积核 `K` 的数值所决定，并以一种高度规律的“对角线重复”模式（托普利茨结构）排列。

------

### **第三部分：逻辑的闭环 —— 雅可比矩阵的转置 与 转置卷积**

现在，我们终于来到了最关键的一步。

根据第一部分的原理，这次卷积操作的反向传播，应该计算 `grad_input` = `J`ᵀ · `grad_output`。

我们来看看 `J` 的转置 `J`ᵀ 长什么样：
$$

\begin{bmatrix}
k_1 & 0 \\
k_2 & k_1 \\
0 & k_2 \\
0 & 0
\end{bmatrix}
$$
现在，我们用它去乘以从下一层传回来的梯度 `grad_output` = `[g₁, g₂]`：
$$
\begin{bmatrix} \text{grad\_s}_1 \\ \text{grad\_s}_2 \\ \text{grad\_s}_3 \\ \text{grad\_s}_4 \end{bmatrix}
=
\begin{bmatrix}
k_1 & 0 \\
k_2 & k_1 \\
0 & k_2 \\
0 & 0
\end{bmatrix}
\begin{bmatrix} g_1 \\ g_2 \end{bmatrix}
$$
让我们仔细观察这个计算结果。它在数学上，**精确地等价于**用一个“翻转”了的卷积核 `[k₂, k₁]` 对一个填充过的梯度 `[g₁, g₂, 0]` 进行一次卷积。而这个操作，正是**“转置卷积 (`ConvTranspose`)”** 的标准定义之一。

至此，我们完成了逻辑的闭环：**对一个卷积操作求反向传播梯度，其严谨的数学实现，就是进行一次转置卷积。**

------

## 实际情景

#### **第一幕：物理世界的退化 (The Physical Degradation)**

1. **真实信号 (`True_Image`)**：在数学上，这张理想的图像可以表示为一个巨大的矩阵，其中只有一个位置的像素值为1（代表那个发光点），其余所有位置都为0。我们称这个矩阵为 `Y`。

2. **串扰的发生**：当 `Y` 的信号投射到相机的CMOS传感器上时，物理串扰发生了。那个值为1的像素点，由于光学和电学效应，其能量会“泄露”到周围的邻居像素上。本来是“鹤立鸡群”的一个点，现在变成了一个中心最亮、向外围逐渐变暗的**“模糊光斑”**。

3. **数学建模**：这个“从点变成斑”的物理过程，在数学上可以被一次**二维卷积** (`Convolution`) 操作完美地描述。那个决定了光斑形状和大小的，就是我们反复讨论的**物理串扰核 `K`**。因此，传感器实际记录到的、无随机噪声的信号 `Ŷ_degraded` 是：
   $$
   \mathbf{X} = \mathbf{K} * \mathbf{Y}
   $$

4. **最终的输入**：相机电路还会引入随机噪声 `N`。所以，您的模型最终得到的**输入图像 `X`** 是：

$$
\mathbf{X} = (\mathbf{K} * \mathbf{Y}) + \mathbf{N}
$$

**至此，我们用数学语言完整地描述了您的项目所面临的“问题”。**

#### **第二幕：神经网络的“猜谜” (The Forward Pass)**

`NewBP+NAFNet`模型接收到这张充满串扰和噪声的图像`X`，它的任务是猜出原始的、只有一个亮点的图像`Y`。

1. **`BaseNet`的“预补偿”**：图像`X`首先进入庞大的`NAFNet`主体。`NAFNet`通过其复杂的网络层，对`X`进行处理。它不知道`K`是什么，也不知道`N`是什么，但它的目标是学习生成一个**中间特征图 `S`**。这个`S`非常关键，它是`NAFNet`主体部分的输出，也是我们NewBP层的输入。也即：
   $$
   S=NAFNet(X)
   $$

2. **NewBP层的“模拟验证”**：`NAFNet`生成的`S`，被送入您放置在网络末端的**NewBP层**。NewBP层的唯一工作，就是用那个固定的、代表物理现实的**串扰核 `K`**，对`S`进行一次卷积。这一步是在模拟：如果`NAFNet`的猜测`S`是正确的‘预补偿’图像，那么当它经过一次真实的物理串扰过程后：
   $$
   \hat{\mathbf{Y}} = \mathbf{K} * \mathbf{S}
   $$

3. **计算损失**：我们用一个损失函数（比如`L1`）来比较模型的最终输出`Ŷ`和我们期望的“标准答案”`Y`之间的差距。
   $$
   L = \mathcal{L}(\mathbf{Y}, \hat{\mathbf{Y}})
   $$

**至此，我们描述了模型如何进行一次“猜测”，并得到一个“分数”。**

#### **第三幕：NewBP的“精准复盘” (The Backward Pass)**

这是整个故事的高潮，也是NewBP算法与您的项目产生深刻联系的地方。现在，损失`L`将化身为梯度，从后向前传播，告诉网络中的每一个参数“你下次应该如何调整，才能猜得更准”。

1. **梯度的起点**：我们首先能得到损失`L`相对于最终输出`Ŷ`的梯度 `∂L/∂Ŷ`。这个梯度告诉我们，最终输出的每个像素应该增加还是减少，才能让损失变小。

2. **关键的十字路口：NewBP层**：现在梯度到达了NewBP层。我们的任务是计算损失`L`相对于NewBP层输入`S`的梯度 `∂L/∂S`。这个梯度将是指导`NAFNet`主体更新的**核心信号**。

3. **调用数学原理**：根据我们在上一次讨论中建立的数学基石，我们知道：
   $$
   \frac{\partial L}{\partial \mathbf{S}} = \left(\frac{\partial \hat{\mathbf{Y}}}{\partial \mathbf{S}}\right)^T \cdot \frac{\partial L}{\partial \hat{\mathbf{Y}}}
   $$
   **数学原理与项目实践的对接**：我们同样知道，对于一个由卷积核`K`定义的卷积操作，其雅可比矩阵的转置`J_NewBP`ᵀ所代表的运算，**在数学上完全等价于一次由同一个卷积核`K`引导的转置卷积 (`ConvTranspose2d`)**。

4. **NewBP的实现**：因此，在您的NewBP层的`backward`函数中，计算`∂L/∂S`这个梯度的具体操作就是：
   $$
   \text{grad}_{\mathbf{S}} = \text{ConvTranspose2d}(\text{grad}_{\hat{\mathbf{Y}}}, \mathbf{K})
   $$

**至此，我们用严谨的数学步骤，将NewBP思想转化为了您项目中的一个具体计算操作。**

#### **第四幕：`BaseNet`的“顿悟”**

`NAFNet`主体接收到了这份由NewBP“精准复盘”后得到的梯度`∂L/∂S`。这份梯度与普通梯度有何不同？

- **普通梯度**会模糊地告诉`NAFNet`：“你的输出`S`有问题，调整一下吧。”
- **NewBP梯度**则会精确地告诉`NAFNet`：“你的输出`S`，在经过与`K`的卷积后，产生了最终的误差。现在这份梯度，已经通过`K`的转置卷积，包含了‘如何抵消`K`的影响’的精确信息。请据此调整你的参数。”

经过成千上万次的迭代，`NAFNet`在NewBP的指导下，最终会学会：为了让最终的`Ŷ`无限接近只有一个亮点的`Y`，它必须生成一个奇特的中间特征图`S`——这个`S`可能看起来像一个**中心为亮点、周围环绕着一圈负值“光环”**的图像。因为只有这样一个经过“预补偿”的`S`，在经过与模糊核`K`的卷积后，那圈负值“光环”才能完美地抵消掉卷积带来的模糊效应，最终还原出一个清晰、锐利的单点。

**这就是您的`NewBP+NAFNet`模型，从物理到数学，再回到模型学习的完整工作闭环。**

