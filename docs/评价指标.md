### **文档：低光照图像去噪任务的定量评估指标详解**

本文件旨在详细阐述在低光照图像去噪项目中，用于定量评估模型性能的各项核心指标。这些指标分为两大类：**图像质量评估指标**和**模型效率评估指标**。

#### **第一部分：图像质量评估指标**

此类指标用于衡量模型输出的去噪图像与“标准答案”（Ground Truth）之间的保真度与感知相似度。

---

##### **1. PSNR (Peak Signal-to-Noise Ratio, 峰值信噪比)**

* **核心思想**:
    PSNR是图像处理领域最传统、最广泛使用的保真度度量指标。它建立在**均方误差（Mean Squared Error, MSE）**的基础之上，通过计算对应像素点之间的误差来衡量图像的失真程度。其核心假设是：像素误差越小，图像质量越高。

* **计算方式**:
    1.  首先，计算去噪图像 `Ŷ` 与真实图像 `Y` 之间的均方误差MSE：
        $$
        MSE = \frac{1}{H \times W \times C} \sum_{i=1}^{H} \sum_{j=1}^{W} \sum_{k=1}^{C} (Y_{(i,j,k)} - \hat{Y}_{(i,j,k)})^2
        $$
        其中 `H, W, C` 分别是图像的高、宽和通道数。
    2.  然后，根据MSE计算PSNR值：
        $$
        PSNR = 10 \cdot \log_{10}\left(\frac{MAX_I^2}{MSE}\right)
        $$
        其中，$MAX_I$ 是图像像素值的最大可能值。对于8位图像，它是255；对于归一化到 `[0, 1]` 的浮点图像，它是1.0。

* **指标解读**:
    * PSNR的单位是分贝（dB）。
    * **数值越高越好**，表示去噪图像与真实图像的像素级差异越小，失真度越低。
    * **局限性**: PSNR只关注像素值的绝对差异，有时其结果与人眼的主观感知质量并不完全一致。例如，一张轻微模糊但无噪点的图像，其PSNR可能低于一张锐利但有少量伪影的图像。

* **实现要点 (用于后续生成代码)**:
    * 输入：两张尺寸和数据类型完全相同的图像（通常为PyTorch Tensor或NumPy Array）。
    * 数据范围（Data Range）必须明确指定（例如255或1.0）。
    * 可使用`torchmetrics`、`skimage.metrics.peak_signal_noise_ratio`等库函数直接计算。

---

##### **2. SSIM (Structural Similarity Index Measure, 结构相似性)**

* **核心思想**:
    SSIM的设计初衷是为了弥补PSNR不关心图像结构的缺陷。它认为人眼视觉系统主要关注场景中的结构信息。因此，SSIM从**亮度（Luminance）**、**对比度（Contrast）**和**结构（Structure）**三个维度来综合评估两张图像的相似性。

* **计算方式**:
    SSIM在图像上使用一个滑动窗口进行计算，对每个窗口内的图像块计算三项指标，然后综合得出最终得分。
    1.  **亮度比较**: 基于图像块的平均灰度值。
    2.  **对比度比较**: 基于图像块的灰度标准差。
    3.  **结构比较**: 基于图像块的灰度协方差。
    最终的SSIM值是这三个分量的加权乘积。

* **指标解读**:
    * SSIM的取值范围为 `[-1, 1]`。
    * **数值越接近1越好**，表示两张图像在结构上越相似。值为1代表两张图像完全相同。
    * 相比PSNR，SSIM通常能更好地反映人眼对图像质量的主观判断。

* **实现要点**:
    * 输入：两张尺寸相同的图像。
    * 需要指定数据范围（Data Range）。
    * 滑动窗口大小（`win_size`）是一个可调参数，通常为奇数，如7或11。
    * 可使用`torchmetrics`、`skimage.metrics.structural_similarity`等库函数。

---

##### **3. LPIPS (Learned Perceptual Image Patch Similarity, 学习感知图像块相似度)**

* **核心思想**:
    LPIPS是近年来被广泛采用的、更先进的感知度量指标。它不再依赖于手工设计的公式，而是利用一个**预训练的深度神经网络**（如AlexNet, VGG）来模仿人类的视觉感知系统。其核心思想是：如果两张图像在人眼看来是相似的，那么它们在深度网络的不同层级上提取出的**深层特征**也应该是相似的。

* **计算方式**:
    1.  将去噪图像 `Ŷ` 和真实图像 `Y` 分别输入到一个固定的、预训练好的深度网络中。
    2.  从网络的多个卷积层中提取出各自的特征图（Feature Maps）。
    3.  对每个层级的特征图进行归一化处理。
    4.  计算`Ŷ`和`Y`在对应层级上特征图之间的L2距离（欧氏距离）。
    5.  将所有层级的距离进行加权求和，得到最终的LPIPS分数。

* **指标解读**:
    * **数值越低越好**，表示两张图像在感知上越相似。
    * LPIPS对于模糊、纹理失真、伪影等问题非常敏感，能极好地衡量生成图像的**真实感**和**细节还原能力**，这与你项目中“细节保留”的目标高度契合。

* **实现要点**:
    * 需要安装专门的`lpips`库。
    * 输入图像（PyTorch Tensor）需要被归一化到 `[-1, 1]` 的范围内。
    * 输入张量的形状通常为 `(N, C, H, W)`。
    * 需要指定使用的网络骨干（如`'alex'`或`'vgg'`）。

#### **第二部分：模型效率评估指标**

此类指标用于衡量模型本身的计算成本和存储需求，是评估模型实用性的关键。

---

##### **4. 参数量 (Parameters)**

* **核心思想**:
    指模型中所有**可训练参数**（权重和偏置）的总数量。它直接决定了模型文件的大小和对存储空间的需求。

* **计算方式**:
    遍历模型的所有层，累加每一层可训练参数张量中的元素数量。

* **指标解读**:
    * 单位通常是“百万”（M）。
    * **数值越低越好**（在性能相近的前提下），代表模型更轻量，更易于部署。

* **实现要点**:
    * 在PyTorch中，可以通过 `sum(p.numel() for p in model.parameters() if p.requires_grad)` 来计算。

---

##### **5. FLOPs (Floating Point Operations, 浮点运算次数)**

* **核心思想**:
    指模型完成一次**前向传播**所需要的浮点运算（通常是乘法和加法）的总次数。它衡量的是模型的**计算复杂度**，与模型的推理速度高度相关。

* **计算方式**:
    这个指标无法手动简单计算，需要借助专门的库（如`fvcore`, `thop`）来分析模型的计算图。这些库会“钩住”模型中的每个操作（如卷积、全连接），并根据其输入输出尺寸累加其对应的FLOPs。

* **指标解读**:
    * 单位通常是“GFLOPs”（$10^9$次浮点运算）。
    * **数值越低越好**，代表模型计算效率更高。
    * 注意：FLOPs是**与输入图像尺寸相关**的，在报告时必须注明测试所用的图像分辨率。

* **实现要点**:
    * 需要提供一个与实际推理时尺寸相同的**样本输入张量（dummy input）**，以便库函数追踪计算图。

---

##### **6. 单图平均推理时间 (Average Inference Time)**

* **核心思想**:
    直接测量模型在**特定硬件**（如某型号的GPU）上处理单张图像所需的时间。这是衡量模型**实际运行速度**的最直观指标。

* **计算方式**:
    1.  将模型设置为评估模式 (`model.eval()`)。
    2.  在`torch.no_grad()`环境下运行。
    3.  进行若干次“热身”（warm-up）推理，以稳定GPU时钟频率。
    4.  循环处理一批测试图像，使用`torch.cuda.Event`或`time.time()`并配合`torch.cuda.synchronize()`来精确计时。
    5.  计算总时间除以图像数量，得到平均时间。

* **指标解读**:
    * 单位通常是“毫秒/图”（ms/image）。
    * **数值越低越好**。
    * **必须注明测试硬件**（如NVIDIA RTX 4090），因为该指标高度依赖于硬件平台。

* **实现要点**:
    * 必须在GPU上进行计时，并使用`torch.cuda.synchronize()`来确保CPU和GPU操作同步，避免异步执行带来的计时误差。
    * 关闭梯度计算是必须的。

