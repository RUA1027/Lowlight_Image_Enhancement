# 实验设计



### **实验设计计划：基于物理感知的低光照图像去噪模型验证**



#### **1. 研究目标与核心假设**



本实验旨在设计并验证一个基于物理感知的深度学习去噪网络（`NewBP-Net`）的有效性。该网络通过将图像传感器的像素串扰模型，利用NewBP算法思想嵌入到一个先进的神经网络架构中，以期在去除低光照噪声的同时，实现对图像微小细节的卓越保留能力。

**核心假设 (Hypotheses):**

- **H1 (有效性假设)**：在相同的底层网络架构下，集成了物理串扰模型的 `NewBP-Net` 在各项图像质量指标上，将显著优于未集成该模块的原始基线模型。
- **H2 (先进性假设)**：`NewBP-Net` 的性能，尤其是在如LPIPS等感知度量和定性视觉效果上，将与当前顶尖的纯数据驱动去噪模型（SOTA models）持平或更优。
- **H3 (参数敏感性假设)**：物理串扰模型的作用范围（即二维卷积核尺寸）是影响模型性能的一个关键超参数，且采用先进的损失函数将进一步放大 `NewBP-Net` 在细节保留上的优势。



#### **2. 模型构建与实验分组**



为验证上述假设，我们将设立结构清晰的实验组与对照组。所有模型将基于**同一个选定的底层神经网络架构**==NAFNet==进行构建，以确保对比的公平性。

> [!TIP]
>
> **Q：为何选择NAFNet为基线？**
>
> **理由：**
>
> - 架构分析：NAFNet是近年来一颗耀眼的新星，它代表了现代CNN架构的简洁与高效。其宏观上依然是一个U-Net形态的架构，但它通过一系列巧妙的简化（如移除非线性激活函数、使用简化的通道注意力和层归一化）达成了一个惊人的平衡。
> - 性能定位：顶级的SOTA，尤其是在您的目标任务上。根据其官方报告和多个第三方验证，NAFNet在SID数据集上的PSNR达到了40.30 dB，超越了包括Restormer在内的诸多强手，而且其计算成本远低于许多竞争者。
> - 兼容性与作用：极高。由于其本质上是一个U-Net架构，集成您的CCL层几乎和在原生U-Net上一样简单。它既拥有U-Net的结构优势（跳跃连接），又具备了SOTA级别的性能和高效率。这使其成为您实验组（即搭载NewBP模块的主力模型）的理想选择。
> - 它在您的目标数据集（SID）上性能最强。
> - 它的高效率和高兼容性，能让您将主要精力聚焦在NewBP模块本身的打磨上，而非被复杂的模型实现所困扰。
> - 使用 NewBP + NAFNet 作为您的最终模型，去挑战其他所有SOTA，这是最有力的叙事。

------

- **实验组 (Experimental Groups): `NewBP-NAFNet` 系列**
  - **目标**：验证 `NewBP` 模块的有效性，并探索物理串扰核尺寸对 `NAFNet` 这一顶级 `BaseNet` 的性能影响。
  - **Group A (`NewBP-NAFNet-3x3 全色卷积核`)**: 选择 **NAFNet** 作为 `BaseNet` + 集成3x3串扰核的CCL层。
  - **Group B (`NewBP-NAFNet-3x3 RGB混合卷积核`)**: 选择 **NAFNet** 作为 `BaseNet` + 集成3x3串扰核的CCL层。
- **对照组 (Control Groups):**
  - **目标**：提供多层次性能基准，剥离并验证 `NewBP` 模块的真实贡献，并衡量模型与领域内经典及SOTA模型的差距。
  - **Group D (黄金对照, `NAFNet`)**: **原始的、未做任何修改的 `NAFNet` 模型**。该组是直接衡量实验组A,B,C中 `NewBP` 模块带来多大提升的**最关键**参照物。
  - **Group F (经典基线对照, `U-Net`)**: 选择经典的 `U-Net` 模型，以衡量您的方案相较于该领域奠基性架构的性能进步幅度。



#### **3. 核心模块参数设定 (CCL)**



- **物理参数来源**：实验组A, B, C中所用CCL层的二维卷积核**具体数值**，将优先通过**文献调研**获取。我们将查找对SID数据集中所用相机传感器（或同类型CMOS传感器）进行过物理特性表征（如点扩散函数PSF、串扰分析）的权威学术论文，并直接引用其报告的参数作为我们**固定的、不可训练的**物理参数。该参数的来源将在最终报告中被明确标注和引用。



#### **4. 训练策略与损失函数**



- **通用训练条件**：为确保公平性，所有分组（A-F）的模型都将在SID数据集上，使用完全相同的训练策略进行端到端训练，包括但不限于：总训练回合数 (Epochs)、批量大小 (Batch Size)、优化器 (Optimizer)、初始学习率及衰减策略、以及随机种子 (Random Seed)。
- **损失函数策略**：
  1. **主要实验**：所有模型的主要训练将采用一种**混合损失函数**，以平衡像素级准确性和感知真实性。其形式为 `L_total = λ₁ * L_perceptual + λ₂ * L₁`。权重系数 `λ₁` 和 `λ₂` 在所有实验中保持一致。
  2. **损失函数消融实验**：在主要实验结束后，选取表现最佳的实验组模型（如 `NewBP-Net-5x5`），单独使用传统的 `L₁` 损失函数重新进行一次训练。通过对比两次训练的结果，以量化先进损失函数对细节保留能力的具体贡献。



#### **5. 实验平台与数据集**



- **数据集**：采用领域公认的 **SID (See-in-the-Dark)** 数据集。其提供的RAW格式图像数据，最大限度地保留了传感器层面的物理噪声特性，是验证本研究物理感知思想的理想平台。
- **硬件与软件**：所有模型的训练和测试都将在统一的硬件平台和软件环境下进行，以确保推理速度等性能指标的可比性。



#### **6. 评估方法与衡量指标**



我们将从定量、定性及可解释性三个维度，对所有模型的性能进行综合评估。

- **定量评估**：制作一张总览性能的表格，对比所有模型在SID测试集上的表现。
  - **图像质量指标**：
    - PSNR (峰值信噪比，越高越好)
    - SSIM (结构相似性，越接近1越好)
    - LPIPS (感知相似度，越低越好)
  - **模型成本指标**：
    - 参数量 (Params, M，越低越好)
    - 计算量 (FLOPs, G，越低越好)
    - 单图平均推理时间 (ms，越低越好)
- **定性评估**：
  - 选取若干具有挑战性的测试样本（如包含精细纹理、微弱文字、复杂边缘的场景），进行并列的视觉效果对比。
  - 对关键区域进行**局部放大 (Zoom-in)**，直观地展示`NewBP-Net`在细节保留和伪影抑制方面的优势。
- **可解释性分析 (XAI)**：
  - **特征图可视化**：对比`NewBP-Net`的CCL层输出与`BaseNet-Vanilla`第一层网络输出的特征图，以可视化证明CCL层对系统性噪声的“预处理”效果。
  - **残差图分析**：通过对比不同模型的残差图（“标准答案” - “模型输出”），分析其误差分布。期望`NewBP-Net`的残差更接近纯粹的随机噪声，而其他模型的残差中可能保留着未被有效处理的“结构性”误差。

## 模型集成方法



### **阶段一：理论构建 —— 搭建从NewBP到`BaseNet`的桥梁**



在动手之前，我们必须确保理论根基是牢固的。这里的核心是确认，将NewBP应用于您的`BaseNet`输入端，在数学上是完全成立的。

1. **重申NewBP的核心**：NewBP算法的核心，是为那些“一个输出点同时是多个输入点的函数”的层，提供一个正确的梯度计算法则。在反向传播时，一个输入点的梯度，应该等于**所有**受它影响的输出点的梯度的加权和。这个“加权和”的过程，在数学上由**雅可比矩阵的转置** (`J^T`) 来描述。
2. **确认`BaseNet`的入口是适用场景**：
   - 您的`BaseNet`（无论是U-Net还是NAFNet），其入口处都是一个标准的**二维卷积层 (`Conv2d`)**。
   - 一个`Conv2d`操作，完美符合NewBP的核心前提：经过卷积后，输出特征图上的**每一个像素点**的值，都取决于输入图像上一个**局部邻域内所有像素点**的值（这个邻域的大小由卷积核尺寸决定）。
   - 因此，`Conv2d`层就是一个需要应用NewBP思想的完美场景。
3. **建立数学等价关系（关键一步）**：
   - 在NewBP原文中，描述物理串扰的数学工具是**雅可比矩阵 `J`**。
   - 在您的项目中，描述像素串扰的物理模型是**二维卷积核 `K`**。
   - 这里的核心洞察是：对于一个`Conv2d`操作，其对应的雅可比矩阵`J`是一个具有特定结构的“托普利茨矩阵”，而对这个雅可比矩阵进行转置 (`J^T`)，在数学上**完全等价于**进行一次**二维转置卷积 (`ConvTranspose2d`)** 操作，且使用的卷积核正是原始的卷积核`K`。

**结论**：我们找到了将NewBP思想集成到`BaseNet`中的关键钥匙——**重写`BaseNet`入口卷积层的反向传播过程，将其标准的梯度计算，替换为一个使用相同物理串扰核的`ConvTranspose2d`操作**。这在数学上是严谨的，完全没有违背NewBP的核心思想。

------



### **阶段二：代码实现 —— 打造您的“串扰补偿层 (CCL)”**



现在，我们将上述理论转化为具体的代码。在PyTorch这样的现代深度学习框架中，我们可以通过自定义一个`autograd.Function`来为网络层注入我们自己的梯度计算逻辑。

这个过程分为两步：定义一个自定义的“梯度计算器”，然后用它来构建您的CCL层。

**第一步：创建`NewBP_Gradient_Calculator` (继承`torch.autograd.Function`)**

这是实现NewBP算法的核心。这个特殊的类需要我们手动定义它的`forward`和`backward`过程。

Python

```python
import torch
import torch.nn.functional as F

# 假设 self.crosstalk_kernel 是您事先加载好的、固定的物理串扰卷积核
# 例如：torch.tensor([[0.01, 0.02, 0.01], [0.02, 0.88, 0.02], [0.01, 0.02, 0.01]])

class NewBP_Gradient_Calculator(torch.autograd.Function):

    @staticmethod
    def forward(ctx, input_tensor, crosstalk_kernel):
        """
        前向传播：就是一次标准的二维卷积。
        我们在这里不保存任何中间变量 (ctx.save_for_backward)，因为反向传播不依赖于它们。
        """
        # 使用F.conv2d进行卷积，padding='same'确保输出尺寸不变
        return F.conv2d(input_tensor, crosstalk_kernel, padding='same')

    @staticmethod
    def backward(ctx, grad_output):
        """
        反向传播：这是NewBP的核心所在！
        我们不使用标准的链式法则，而是直接实现NewBP的梯度公式。
        """
        # 从上下文中获取在前向传播时使用的串扰核
        # (注意：实际实现时需要找到一种方法传递kernel，这里为了清晰简化了)
        crosstalk_kernel = ctx.crosstalk_kernel 

        # NewBP的梯度计算，等价于一次转置卷积
        # grad_output是下一层传回的梯度
        grad_input = F.conv_transpose2d(grad_output, crosstalk_kernel, padding='same')

        # 因为crosstalk_kernel是固定的，所以它自身的梯度是None
        return grad_input, None
```

**第二步：构建`CrosstalkCompensationLayer` (继承`torch.nn.Module`)**

这是一个标准的神经网络层，它会使用我们上面定义的“梯度计算器”来完成运算。

Python

```python
import torch.nn as nn

class CrosstalkCompensationLayer(nn.Module):
    def __init__(self, physical_kernel):
        super(CrosstalkCompensationLayer, self).__init__()
        
        # 将物理串扰核作为模块的一个固定参数（buffer）
        # buffer意味着它属于模块状态，但不是模型的可训练参数
        self.register_buffer('crosstalk_kernel', physical_kernel)

    def forward(self, x):
        # 在前向传播时，调用我们自定义的梯度计算器
        # 我们需要将self.crosstalk_kernel传递给它，以便在backward时使用
        NewBP_Gradient_Calculator.ctx.crosstalk_kernel = self.crosstalk_kernel
        return NewBP_Gradient_Calculator.apply(x, self.crosstalk_kernel)
```

------



### **阶段三：模型整合 —— 将CCL无缝嵌入`BaseNet`**



这是最后一步，也是最直接的一步。

1. **加载您的`BaseNet`**：实例化一个原始的`NAFNet`或`U-Net`模型。
2. **加载您的物理参数**：根据您的文献调研，创建一个`torch.Tensor`来存储您的物理串扰卷积核 `physical_kernel`。
3. **实例化您的CCL层**：`ccl_layer = CrosstalkCompensationLayer(physical_kernel)`。
4. **执行“替换手术”**：
   - 找到`BaseNet`的第一个卷积层。在大多数模型中，它通常被称为`model.conv_in`或`model.input_conv`之类的名字。您需要阅读`BaseNet`的源码来确定它的具体名称。
   - 用您的CCL层替换掉它。例如：`nafnet_model.conv_in = ccl_layer`。
5. **完成**：现在您得到了一个全新的、集成了NewBP思想的`NewBP-NAFNet`模型。您可以像训练任何一个标准模型一样去训练它。在训练过程中，当反向传播到达入口层时，PyTorch会自动调用您编写的`NewBP_Gradient_Calculator.backward`方法，从而实现了您的设计，而模型的其余部分则完全不受影响，继续使用标准的BP算法。

通过以上三个阶段，我们**在没有改变NewBP数学原理的前提下，严谨地、完整地**将您的物理感知思想集成到了一个先进的`Base-Net`中，为后续的实验打下了坚实的基础。